{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ur8xi4C7S06n"
   },
   "outputs": [],
   "source": [
    "# Copyright 2023 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JAPoU8Sm5E6e"
   },
   "source": [
    "# Perguntas e respostas com modelos genertivos na Vertex AI\n",
    "\n",
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/generative-ai/blob/main/language/intro_palm_api.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Colab logo\"> Execute no Colab\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://github.com/GoogleCloudPlatform/generative-ai/blob/main/language/intro_palm_api.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
    "      Veja no GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/generative-ai/blob/main/language/intro_palm_api.ipynb\">\n",
    "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\">\n",
    "      Execute no Vertex AI Workbench\n",
    "    </a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tvgnzT1CKxrO"
   },
   "source": [
    "## Visão geral\n",
    "\n",
    "Grandes modelos de linguagem podem ser usados para várias tarefas de processamento de linguagem natural, incluindo perguntas e respostas (Q&A). Esses modelos são treinados em uma grande quantidade de dados de texto e podem gerar respostas de alta qualidade para uma ampla gama de perguntas. Uma coisa a observar aqui é que a maioria dos modelos tem datas limite em relação ao seu conhecimento, e perguntar qualquer coisa muito recente pode resultar em uma resposta incompleta, imaginativa ou incorreta (ou seja, uma alucinação).\n",
    "\n",
    "Este bloco de anotações aborda os fundamentos dos prompts para responder a perguntas usando um modelo generativo. Além disso, apresenta o `domínio aberto` (conhecimento disponível na internet pública) e o `domínio fechado` (conhecimento que é mais privado - normalmente conhecimento empresarial ou pessoal).\n",
    "\n",
    "Saiba mais sobre o design de prompt na [documentação oficial](https://cloud.google.com/vertex-ai/docs/generative-ai/text/text-overview#prompt_structure)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d975e698c9a4"
   },
   "source": [
    "### Objetivo\n",
    "\n",
    "Ao final deste notebook, você será capaz de escrever prompts para os seguintes cenários:\n",
    "\n",
    "* Perguntas de **domínio aberto**:\n",
    "     * Solicitação one-shot\n",
    "     * Solicitação few-shot\n",
    "\n",
    "\n",
    "* Perguntas de **domínio fechado**:\n",
    "     * Fornecer conhecimento personalizado como contexto\n",
    "     * Instrução de ajuste das saídas\n",
    "     * Solicitação few-shot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custos\n",
    "Este tutorial usa os seguintes componentes de Google Cloud:\n",
    "\n",
    "* Vertex AI Generative AI Studio\n",
    "\n",
    "Saiba mais sobre possíveis custos envolvidos [preços da Vertex AI](https://cloud.google.com/vertex-ai/pricing),\n",
    "e use a [Calculadora de preços](https://cloud.google.com/products/calculator/)\n",
    "para gerar uma estimativa de custo com base no uso projetado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QDU0XJ1xRDlL"
   },
   "source": [
    "## Primeiros Passos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2a5AEr0lkLKD"
   },
   "source": [
    "### Instalando os SDK da Vertex AI e da Cloud Translate API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "82ad0c445061",
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install google-cloud-aiplatform google-cloud-translate --upgrade --user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Somente Colab:** Descomente a célula a seguir para reiniciar o kernel ou use o botão para reiniciar o kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_Hsqwn4hkLKE"
   },
   "outputs": [],
   "source": [
    "# # Reinicia automaticamente o kernel após as instalações para que seu ambiente possa acessar os novos pacotes\n",
    "# import IPython\n",
    "\n",
    "# app = IPython.Application.instance()\n",
    "# app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xe7OuYuGkLKF"
   },
   "source": [
    "### Autenticando seu ambiente de notebook\n",
    "* Se você estiver usando o **Colab** para executar este notebook, descomente a célula abaixo e continue.\n",
    "* Se você estiver usando o **Vertex AI Workbench**, confira as instruções de configuração [aqui](../setup-env/README.md)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U9Gx2SAZkLKF"
   },
   "outputs": [],
   "source": [
    "# from google.colab import auth\n",
    "# auth.authenticate_user()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "960505627ddf"
   },
   "source": [
    "### Importando as bibliotecas necessárias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Somente Colab:** Descomente a célula a seguir para realizar o processo adequado de inicialização da SDK da Vertex AI.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import vertexai\n",
    "\n",
    "# PROJECT_ID = \"[seu-project-id]\"  # @param {type:\"string\"}\n",
    "# vertexai.init(project=PROJECT_ID, location=\"us-central1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PyQmSRbKA8r-"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from vertexai.preview.language_models import TextGenerationModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UP76a2la7O-a"
   },
   "source": [
    "#### Carregando o modelo `text-bison`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7isig7e07O-a"
   },
   "outputs": [],
   "source": [
    "generation_model = TextGenerationModel.from_pretrained(\"text-bison@001\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Criando a função *wrapper* para utilizar os modelos em Português\n",
    "\n",
    "Até o momento desde treinamento, as API do Generative AI Studio suportam somente interações no idioma inglês. Para fazermos interações utilizando o idioma português, vamos utilizar a [Cloud Translation API](https://cloud.google.com/translate) para traduzir as nossas solicitações do português para o inglês e as respostas da API de inglês para português."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import translate\n",
    "\n",
    "project_id = !gcloud config list project\n",
    "project_id = project_id[1].split('=')[1].strip()\n",
    "parent = f'projects/' + project_id\n",
    "\n",
    "\n",
    "def traduza(texto, idioma_destino):\n",
    "    client = translate.TranslationServiceClient()\n",
    "\n",
    "    response = client.translate_text(\n",
    "        parent=parent,\n",
    "        contents=[texto],\n",
    "        target_language_code=idioma_destino,\n",
    "        mime_type=\"text/plain\"\n",
    "    )\n",
    "\n",
    "    return response.translations[0].translated_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fIPcn5dZ7O-b"
   },
   "source": [
    "## Perguntas e respostas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cNNEz7vGFYUP"
   },
   "source": [
    "Os recursos de resposta a perguntas exigem o fornecimento de um prompt ou uma pergunta que o modelo possa usar para gerar uma resposta. O prompt pode ser algumas palavras ou algumas frases completas, dependendo da complexidade da pergunta.\n",
    "\n",
    "Ao criar um prompt de resposta a perguntas, é essencial ser específico e fornecer o máximo de contexto possível. Isso ajuda o modelo a entender a intenção por trás da pergunta e gerar uma resposta relevante. Por exemplo, se você quiser perguntar:\n",
    "\n",
    "```\n",
    "\"Qual é a capital da França?\",\n",
    "\n",
    "então um bom prompt poderia ser:\n",
    "\n",
    "\"Diga-me o nome da cidade que é capital da França.\"\n",
    "\n",
    "```\n",
    "\n",
    "Além de ser específico, o prompt também deve ser gramaticalmente correto e livre de erros ortográficos. Isso ajuda o modelo a gerar uma resposta fácil de entender e contém menos erros ou imprecisões.\n",
    "\n",
    "Ao fornecer prompts específicos e ricos em contexto, você pode ajudar o modelo a entender a intenção por trás da pergunta e gerar respostas precisas e relevantes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C5N9ZnlECm-z"
   },
   "source": [
    "Abaixo estão algumas diferenças entre as categorias **domínio aberto** e **domínio fechado** para prompts de resposta a perguntas.\n",
    "\n",
    "* **Domínio aberto**: Todas as perguntas cujas respostas já estão disponíveis online. Eles podem pertencer a qualquer categoria, como história, geografia, países, política, química, etc. Isso inclui perguntas triviais ou de conhecimento geral, como:\n",
    "\n",
    "```\n",
    "P: Quem ganhou o ouro olímpico na natação?\n",
    "P: Quem é o presidente de [dado país]?\n",
    "P: Quem escreveu [livro específico]\"?\n",
    "```\n",
    "\n",
    "Lembre-se do corte de treinamento de modelos generativos, pois perguntas envolvendo informações mais recentes do que aquelas com as quais o modelo foi treinado podem fornecer respostas incorretas ou imaginativas.\n",
    "\n",
    "\n",
    "* **Domínio fechado**: Se você possui alguma base de conhecimento interna não disponível na internet pública, então ela pertence à categoria _domínio fechado_.\n",
    "Você pode passar esse conhecimento \"privado\" como contexto para o modelo. Se solicitado corretamente, é mais provável que o modelo responda dentro do contexto fornecido e menos provável que dê respostas além da Internet aberta.\n",
    "\n",
    "Considere o exemplo de criar um bot de perguntas e respostas sobre a documentação interna do produto. Nesse caso, você pode passar a documentação completa para o modelo e solicitar que ele responda apenas com base nisso.\n",
    "\n",
    "Prompt típico para **domínio fechado**:\n",
    "\n",
    "```\n",
    "Prompt: f\"\"\" Resposta do contexto abaixo: \\n\\n\n",
    "contexto: {sua base de conhecimento} \\n\n",
    "pergunta: {pergunta específica para essa base de conhecimento} \\n\n",
    "resposta: {a ser previsto pelo modelo} \\n\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "Abaixo estão alguns exemplos para entender esses diferentes tipos de prompts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WBoN6zixDSiX"
   },
   "source": [
    "### Domínio aberto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wJnv8XhnDXQm"
   },
   "source": [
    "#### Prompt zero-shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PaYoQuRwCm-z"
   },
   "outputs": [],
   "source": [
    "prompt = traduza(\"\"\"Pregunta: ¿Quién fue el presidente de Argentina en 2010?\\n\n",
    "                    Respuesta:\n",
    "                     \"\"\", \"en\")\n",
    "print(\n",
    "    traduza(generation_model.predict(prompt, max_output_tokens=256, temperature=0.1).text, \"es\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4qcUdUgwCm-z"
   },
   "outputs": [],
   "source": [
    "prompt = traduza(\"\"\"Pregunta: ¿Cuál es la montaña más alta del mundo?\\n\n",
    "                     Respuesta:\n",
    "         \"\"\", \"en\")\n",
    "print(\n",
    "    traduza(generation_model.predict(prompt, max_output_tokens=20, temperature=0.1).text, \"es\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HShw52X2Dcmx"
   },
   "source": [
    "#### Prompt few-shot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tj_2hHAWE8vh"
   },
   "source": [
    "Digamos que você queira obter uma resposta curta do modelo (como apenas um nome específico). Para fazer isso, você pode aproveitar um prompt few-shot e fornecer exemplos ao modelo para ilustrar o comportamento esperado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RE5yCAaqDg7m"
   },
   "outputs": [],
   "source": [
    "prompt = traduza(\"\"\"Pregunta: Quem é atualmente o presidente da França?\\n\n",
    "                Respuesta: Emmanuel Macron \\n\\n\n",
    "\n",
    "                Pregunta: Quem inventou o telefone? \\n\n",
    "                Respuesta: Alexander Graham Bell \\n\\n\n",
    "\n",
    "                Pregunta: Quem escreveu o livro \"1984\"?\n",
    "                Respuesta: George Orwell\n",
    "\n",
    "                Pregunta: Quem descobriu a penicilina?\n",
    "                Respuesta:\n",
    "                \"\"\", \"en\")\n",
    "print(\n",
    "    traduza(generation_model.predict(prompt, max_output_tokens=20, temperature=0.1).text, \"es\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xGvs0jFsUlvM"
   },
   "source": [
    "#### Prompt zero-shot vs few-shot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7yjsAMuMUfZC"
   },
   "source": [
    "O prompt zero-shot pode ser útil para gerar texto rapidamente para novas tarefas, mas a qualidade do texto gerado pode ser inferior à de um prompt few-shot com exemplos bem escolhidos. O prompt few-shot geralmente é mais adequado para tarefas que exigem um alto grau de especificidade ou conhecimento específico do domínio, mas requer algum pensamento adicional e potencialmente dados para configurar o prompt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u6UiJTxXEs4t"
   },
   "source": [
    "### Domínio fechado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "03ZITm4AGBvP"
   },
   "source": [
    "#### Adicionando conhecimento como contexto nos prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EkhqjmB6VqPx"
   },
   "source": [
    "Imagine um cenário em que você gostaria de criar um bot de perguntas e respostas que aceita a documentação interna e permite que os usuários façam perguntas sobre ela.\n",
    "\n",
    "No exemplo abaixo, o Google Cloud Storage e a documentação da política de conteúdo são adicionados ao prompt, para que a API PaLM possa usá-lo para responder a perguntas subsequentes com o contexto fornecido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = traduza(\"\"\"\n",
    "Política de almacenamiento y contenido \\n\n",
    "¿Qué tan duraderos son mis datos en Cloud Storage? \\norte\n",
    "El almacenamiento en la nube está diseñado para una durabilidad anual del 99,999999999 % (11 9), que es adecuado incluso para el almacenamiento principal y\n",
    "aplicaciones críticas para el negocio. Este alto nivel de durabilidad se logra mediante la codificación de borrado que almacena fragmentos de datos de forma redundante.\n",
    "en múltiples dispositivos ubicados en múltiples zonas de disponibilidad.\n",
    "Los objetos escritos en Cloud Storage deben almacenarse de forma redundante en al menos dos zonas de disponibilidad diferentes antes de\n",
    "la escritura se reconoce como exitosa. Las sumas de verificación se almacenan y revalidan regularmente para verificar proactivamente que los datos\n",
    "integridad de todos los datos en reposo, así como la detección de corrupción de datos en tránsito. Si es necesario, las correcciones se realizan automáticamente.\n",
    "hecho usando datos redundantes. Los clientes pueden habilitar opcionalmente el control de versiones de objetos para agregar protección contra la eliminación accidental.\n",
    "\"\"\", \"en\")\n",
    "\n",
    "question = traduza(\"¿Cómo podemos lograr una alta disponibilidad?\", \"en\")\n",
    "\n",
    "prompt = traduza(f\"\"\"Responda la siguiente pregunta dado el contexto a continuación:\n",
    "Contexto: {context} \\n\n",
    "Pregunta: {question} \\n\n",
    "Respuesta:\n",
    "\"\"\", \"en\")\n",
    "\n",
    "print(\n",
    "    traduza(generation_model.predict(prompt).text, \"es\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tagWC4VcQIw6"
   },
   "source": [
    "#### Ajustando as instruções de output do modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v9UkogWHXM6N"
   },
   "source": [
    "Outra maneira de ajudar os modelos de linguagem é fornecer instruções adicionais para enquadrar a saída no prompt. Para garantir que o modelo não responda a nada fora do contexto, o prompt pode especificar que a resposta deve ser \"Informações não disponíveis no contexto fornecido\", se for o caso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ouq8FfwSQIBT"
   },
   "outputs": [],
   "source": [
    "question = traduza(\"¿Qué tipo de máquinas se requieren para alojar modelos en Vertex AI?\", \"en\")\n",
    "prompt = traduza(f\"\"\"Responda la siguiente pregunta usando la información disponible como {{Contexto:}}. \\n\n",
    "Si la respuesta no está disponible en {{Contexto:}} y no está seguro del resultado, por favor\n",
    "diga \"Información no disponible en el contexto proporcionado\". \\n\\n\"\"\", \"en\")\n",
    "prompt += traduza(f\"Contexto: {context} \\n\", \"en\")\n",
    "prompt += traduza(f\"Pregunta: {question} \\n\", \"en\")\n",
    "prompt += traduza(\"Respuesta: \", \"en\")\n",
    "\n",
    "print(\"[Respuesta]\")\n",
    "print(\n",
    "    traduza(generation_model.predict(prompt, max_output_tokens=256, temperature=0.3).text, \"es\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iZJfZShPRGqU"
   },
   "source": [
    "#### Prompt few-shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qdSEQeQIS6pt"
   },
   "outputs": [],
   "source": [
    "prompt = traduza(\"\"\"\n",
    "Contexto:\n",
    "El término \"inteligencia artificial\" fue acuñado por primera vez por John McCarthy en 1956. Desde entonces, la IA se ha convertido en un vasto\n",
    "campo con innumerables aplicaciones, desde autos sin conductor hasta asistentes virtuales como Siri y Alexa.\n",
    "\n",
    "Pregunta:\n",
    "¿Qué es la inteligencia artificial?\n",
    "\n",
    "Responder:\n",
    "La inteligencia artificial se refiere a la simulación de la inteligencia humana en máquinas programadas para pensar y aprender como humanos.\n",
    "\n",
    "---\n",
    "\n",
    "Contexto:\n",
    "Los hermanos Wright, Orville y Wilbur, fueron dos pioneros de la aviación estadounidense a quienes se les atribuye la invención y\n",
    "construir el primer avión exitoso del mundo y hacer el primer vuelo más pesado que el aire controlado, propulsado y sostenido por humanos,\n",
    "   el 17 de diciembre de 1903.\n",
    "\n",
    "Pregunta:\n",
    "¿Quiénes eran los hermanos Wright?\n",
    "\n",
    "Responder:\n",
    "Los hermanos Wright fueron pioneros de la aviación estadounidense que inventaron y construyeron el primer avión exitoso del mundo.\n",
    "y realizó el primer vuelo humano controlado, propulsado y sostenido más pesado que el aire el 17 de diciembre de 1903.\n",
    "\n",
    "---\n",
    "\n",
    "Contexto:\n",
    "La Mona Lisa es un retrato del siglo XVI pintado por Leonardo da Vinci durante el Renacimiento italiano. Es uno de\n",
    "las pinturas más famosas del mundo, conocidas por la enigmática sonrisa de la mujer representada en la pintura.\n",
    "\n",
    "Pregunta:\n",
    "¿Quién pintó la Mona Lisa?\n",
    "\n",
    "Responder:\n",
    "\"\"\", \"en\")\n",
    "print(traduza(generation_model.predict(prompt,).text, \"es\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "leYIui80Q4tH"
   },
   "source": [
    "### Perguntas e resposta extrativas\n",
    "\n",
    "No próximo exemplo, o modelo generativo é guiado para entender o significado da pergunta e da passagem e identificar as informações relevantes na passagem que responde à pergunta. O modelo recebe uma pergunta e uma passagem de texto e é solicitado a encontrar a resposta para a pergunta dentro da passagem. A resposta é tipicamente uma frase ou sentença."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bPZqm0QJQ4tH"
   },
   "outputs": [],
   "source": [
    "prompt = traduza(\"\"\"\n",
    "Antecedentes: Hay evidencia de que ha habido cambios significativos en la vegetación de la selva amazónica durante los últimos 21.000 años a través del Último Máximo Glacial (LGM) y la subsiguiente deglaciación.\n",
    "Los análisis de los depósitos de sedimentos de paleolagos de la cuenca del Amazonas y el abanico del Amazonas indican que la precipitación en la cuenca durante el LGM fue menor que en la actualidad, y esto fue casi seguro\n",
    "asociado a la reducida cobertura de vegetación tropical húmeda en la cuenca. Sin embargo, existe un debate sobre cuán extensa fue esta reducción. Algunos científicos argumentan que la selva tropical se ha reducido a pequeños\n",
    "refugios aislados separados por bosques abiertos y pastizales; otros científicos argumentan que la selva tropical permaneció prácticamente intacta, pero se extendió menos hacia el norte, el sur y el este de lo que se ve hoy.\n",
    "Este debate ha resultado difícil de resolver porque las limitaciones prácticas de trabajar en la selva tropical significan que el muestreo de datos se desvía del centro de la cuenca del Amazonas, y ambos\n",
    "Las explicaciones están razonablemente bien respaldadas por los datos disponibles.\n",
    "\n",
    "P: ¿Qué significa LGM?\n",
    "A: Último Máximo Glacial.\n",
    "\n",
    "P: ¿Qué indica el análisis de los depósitos de sedimentos?\n",
    "R: La precipitación en la cuenca durante el LGM fue menor que en la actualidad.\n",
    "\n",
    "P: ¿Cuáles son algunos de los argumentos de los científicos?\n",
    "R: La selva tropical se ha reducido a pequeños refugios aislados separados por bosques abiertos y pastizales.\n",
    "\n",
    "P: ¿Ha habido grandes cambios en la vegetación de la selva amazónica en los últimos cuántos años?\n",
    "R: 21.000.\n",
    "\n",
    "P: ¿Qué causó los cambios en la vegetación de la selva amazónica?\n",
    "A: El Último Máximo Glacial (LGM) y posterior desglaciación\n",
    "\n",
    "P: ¿Qué se analizó para comparar las lluvias amazónicas del pasado y del presente?\n",
    "A: Depósitos de sedimentos.\n",
    "\n",
    "P: ¿Qué se atribuyó a la menor precipitación en la Amazonía durante la LGM?\n",
    "A:\n",
    "\"\"\", \"en\")\n",
    "\n",
    "print(traduza(generation_model.predict(prompt).text, \"es\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "94d80fb55f48"
   },
   "source": [
    "### Avaliação de respostas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b620d23a7634"
   },
   "source": [
    "Você pode avaliar os resultados da tarefa de pergunta e resposta se as respostas de verdade de cada pergunta estiverem disponíveis. No prompt zero-shot, você só pode usar perguntas de 'domínio aberto'. No entanto, com perguntas de 'domínio fechado', você pode adicionar contexto e avaliar de forma semelhante. Para mostrar como isso funciona, comece criando um dataframe simples com perguntas e respostas de verdade."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8e813a463531"
   },
   "outputs": [],
   "source": [
    "qa_data = {\n",
    "    \"pregunta\": [\n",
    "         \"En la barra de direcciones de los navegadores, ¿qué significa 'www'?\",\n",
    "         \"¿Quién fue la primera mujer en ganar un premio Nobel?\",\n",
    "         \"¿Cuál es el nombre del océano más grande de la Tierra?\",\n",
    "     ],\n",
    "     \"answer_groundtruth\": [\"World Wide Web\", \"Marie Curie\", \"El Océano Pacífico\"],\n",
    "}\n",
    "qa_data_df = pd.DataFrame(qa_data)\n",
    "qa_data_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "951a147dc79d"
   },
   "source": [
    "Agora que você tem os dados com perguntas e respostas básicas, pode chamar o modelo de geração PaLM 2 para cada linha de revisão usando a função `apply`. Cada linha usará o prompt dinâmico para prever a resposta usando a API PaLM. Vamos salvar os resultados na coluna `answer_prediction`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ffc47e0cb5b9"
   },
   "outputs": [],
   "source": [
    "def get_answer(row):\n",
    "    prompt = traduza(f\"\"\"Responda las siguientes preguntas con la mayor precisión posible.\\n\\n\n",
    "                     pregunta: {traduza(row, \"en\")}\n",
    "                     respuesta:\n",
    "                     \"\"\", \"en\")\n",
    "    return traduza(generation_model.predict(prompt=prompt).text, \"es\")\n",
    "\n",
    "qa_data_df[\"answer_prediction\"] = qa_data_df[\"pregunta\"].apply(get_answer)\n",
    "qa_data_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6fe997dbf788"
   },
   "source": [
    "Você pode querer avaliar as respostas previstas pela API do PaLM. No entanto, será mais complexo do que a classificação do texto, pois as respostas podem diferir da verdade e podem ser apresentadas em um pouco mais/menos palavras.\n",
    "\n",
    "Por exemplo, você pode observar a pergunta \"Qual é o nome do maior oceano da Terra?\" e veja que o modelo previu \"Oceano Pacífico\" quando um rótulo de verdade é \"O Oceano Pacífico\" com o extra \"O\". Agora, se você usar as métricas de classificação simples, considerará isso uma previsão errada, pois as strings originais e previstas têm uma diferença. No entanto, você pode ver que a resposta está correta, pois um \"The\" extra está causando o problema. É um problema simples de comparação de strings.\n",
    "\n",
    "A solução para a comparação de strings em que `ground_thruth` e `predicted` podem ter algumas letras extras ou menos, uma abordagem é usar um algoritmo de correspondência difusa.\n",
    "A correspondência de string difusa usa [Distância Levenshtein](https://en.wikipedia.org/wiki/Levenshtein_distance) para calcular as diferenças entre duas strings.\n",
    "\n",
    "Aqui um exemplo usando a biblioteca `fuzzywuzzy`, que nos dá a `distância de Levenshtein` entre duas strings, mas em proporção. A pontuação bruta de proporção mede a similaridade da string como um int no intervalo [0, 100]. Para duas strings X e Y, a pontuação é definida por `int(round((2.0 * M / T) * 100))` onde `T` é o número total de caracteres em ambas as strings e `M` é o número de correspondências nas duas strings.\n",
    "\n",
    "Leia mais aqui sobre a [fórmula de proporção](https://anhaidgroup.github.io/py_stringmatching/v0.3.x/Ratio.html):\n",
    "\n",
    "Você pode ver um exemplo para entender melhor.\n",
    "```\n",
    "String1: \"isto é um teste\"\n",
    "String2: \"isto é um teste!\"\n",
    "\n",
    "Razão Fuzz => 97 #\n",
    "\n",
    "Fuzz Partial Ratio => 100 #Como a maioria dos caracteres são iguais e em uma sequência semelhante, o algoritmo calcula a proporção parcial como 100 e ignora adições simples (novos caracteres).\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7b170579a455"
   },
   "source": [
    "Primeiro instale os pacotes `fuzzywuzzy` e `python-Levenshtein`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6c55ea0eaed0"
   },
   "outputs": [],
   "source": [
    "!pip install -q python-Levenshtein --upgrade --user\n",
    "!pip install -q fuzzywuzzy --upgrade --user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5f048152519f"
   },
   "source": [
    "E então calcule o match fuzzy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "040c1f9a175b"
   },
   "outputs": [],
   "source": [
    "from fuzzywuzzy import fuzz\n",
    "\n",
    "\n",
    "def get_fuzzy_match(df):\n",
    "    return fuzz.partial_ratio(df[\"answer_groundtruth\"], df[\"answer_prediction\"])\n",
    "\n",
    "\n",
    "qa_data_df[\"match_score\"] = qa_data_df.apply(get_fuzzy_match, axis=1)\n",
    "qa_data_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "11e266c49860"
   },
   "source": [
    "Agora que você tem a pontuação de correspondência individual (parcial), pode obter a média ou a média de toda a coluna para ter uma noção dos dados gerais.\n",
    "\n",
    "Pontuações próximas a 100 significam que o PaLM 2 pode prever com mais precisão; se a pontuação estiver próxima de 50 ou 0, não teve um bom desempenho."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dae6a92a7650"
   },
   "outputs": [],
   "source": [
    "print(\n",
    "    \"En promedio, la puntuación de todas las predicciones de PaLM 2 fue: \",\n",
    "    qa_data_df[\"match_score\"].mean(),\n",
    "    \" %\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d9e78972cad1"
   },
   "source": [
    "Nesse caso, você obteve um score de mais ou menos 97% como pontuação média, pois algumas previsões faltem algumas palavras. Isso significa que você está muito próximo da verdade básica e algumas respostas estão apenas perdendo a escrita exata da verdade básica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "question_answering.ipynb",
   "toc_visible": true
  },
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-11.m109",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-11:m109"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "local-python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
