{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ur8xi4C7S06n"
   },
   "outputs": [],
   "source": [
    "# Copyright 2023 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JAPoU8Sm5E6e"
   },
   "source": [
    "# Classificação de textos com Generative AI na Vertex AI\n",
    "\n",
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/generative-ai/blob/main/language/intro_palm_api.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Colab logo\"> Execute no Colab\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://github.com/GoogleCloudPlatform/generative-ai/blob/main/language/intro_palm_api.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
    "      Veja no GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/generative-ai/blob/main/language/intro_palm_api.ipynb\">\n",
    "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\">\n",
    "      Execute no Vertex AI Workbench\n",
    "    </a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tvgnzT1CKxrO"
   },
   "source": [
    "## Visão geral\n",
    "\n",
    "Modelos generativos como o PaLM 2 são modelos de linguagem poderosos usados para várias tarefas de processamento de linguagem natural (NLP). Uma delas é a classificação de texto, que envolve a atribuição de uma ou mais categorias a um determinado trecho de texto. Embora a classificação de texto possa ser feita usando técnicas NLP tradicionais, os LLMs podem realizar a classificação fornecendo prompts (em oposição a dados rotulados específicos do domínio), o que pode acelerar o tempo necessário para criar uma solução de classificação de texto. Os modelos de classificação baseados em LLMs podem ser ajustados com muitos exemplos por meio de treinamento de modelo personalizado, mas isso está além do escopo deste notebook.\n",
    "\n",
    "Neste notebook, você explorará como fazer classificação de texto usando prompts com a API PaLM. Saiba mais sobre os prompts de classificação na [documentação oficial](https://cloud.google.com/vertex-ai/docs/generative-ai/text/classification-prompts)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d975e698c9a4"
   },
   "source": [
    "### Objetivo\n",
    "\n",
    "No final do notebook, você será capaz de usar um LLM para executar várias tarefas de classificação, incluindo:\n",
    "\n",
    "* Classificação de texto com prompt zero-shot\n",
    "* Classificação de texto com prompt few-shot\n",
    "* Tarefas como:\n",
    "     * Análise de sentimentos\n",
    "     * Classificação do tópico\n",
    "     * Detecção de spam\n",
    "     * Reconhecimento de intenção\n",
    "     * Identificação do idioma\n",
    "     * Detecção de toxicidade\n",
    "     * Detecção de emoções"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custos\n",
    "Este tutorial usa os seguintes componentes de Google Cloud:\n",
    "\n",
    "* Vertex AI Generative AI Studio\n",
    "\n",
    "Saiba mais sobre possíveis custos envolvidos [preços da Vertex AI](https://cloud.google.com/vertex-ai/pricing),\n",
    "e use a [Calculadora de preços](https://cloud.google.com/products/calculator/)\n",
    "para gerar uma estimativa de custo com base no uso projetado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nSyXtwyz_o_v"
   },
   "source": [
    "## Primeiros Passos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2a5AEr0lkLKD"
   },
   "source": [
    "### Instalando os SDK da Vertex AI e da Cloud Translate API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "82ad0c445061",
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install google-cloud-aiplatform google-cloud-translate --upgrade --user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Somente Colab:** Descomente a célula a seguir para reiniciar o kernel ou use o botão para reiniciar o kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_Hsqwn4hkLKE"
   },
   "outputs": [],
   "source": [
    "# # Reinicia automaticamente o kernel após as instalações para que seu ambiente possa acessar os novos pacotes\n",
    "# import IPython\n",
    "\n",
    "# app = IPython.Application.instance()\n",
    "# app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xe7OuYuGkLKF"
   },
   "source": [
    "### Autenticando seu ambiente de notebook\n",
    "* Se você estiver usando o **Colab** para executar este notebook, descomente a célula abaixo e continue.\n",
    "* Se você estiver usando o **Vertex AI Workbench**, confira as instruções de configuração [aqui](../../setup-env/README.md)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U9Gx2SAZkLKF"
   },
   "outputs": [],
   "source": [
    "# from google.colab import auth\n",
    "# auth.authenticate_user()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "960505627ddf"
   },
   "source": [
    "### Importando as bibliotecas necessárias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Somente Colab:** Descomente a célula a seguir para realizar o processo adequado de inicialização da SDK da Vertex AI.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import vertexai\n",
    "\n",
    "# PROJECT_ID = \"[seu-project-id]\"  # @param {type:\"string\"}\n",
    "# vertexai.init(project=PROJECT_ID, location=\"us-central1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PyQmSRbKA8r-"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from vertexai.preview.language_models import TextGenerationModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UP76a2la7O-a"
   },
   "source": [
    "#### Carregando o modelo `text-bison`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7isig7e07O-a"
   },
   "outputs": [],
   "source": [
    "generation_model = TextGenerationModel.from_pretrained(\"text-bison@001\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Criando a função *wrapper* para utilizar os modelos em Português\n",
    "\n",
    "Até o momento desde treinamento, as API do Generative AI Studio suportam somente interações no idioma inglês. Para fazermos interações utilizando o idioma português, vamos utilizar a [Cloud Translation API](https://cloud.google.com/translate) para traduzir as nossas solicitações do português para o inglês e as respostas da API de inglês para português."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import translate\n",
    "\n",
    "project_id = !gcloud config list project\n",
    "project_id = project_id[1].split('=')[1].strip()\n",
    "parent = f'projects/' + project_id\n",
    "\n",
    "\n",
    "def traduza(texto, idioma_destino):\n",
    "    client = translate.TranslationServiceClient()\n",
    "\n",
    "    response = client.translate_text(\n",
    "        parent=parent,\n",
    "        contents=[texto],\n",
    "        target_language_code=idioma_destino,\n",
    "        mime_type=\"text/plain\"\n",
    "    )\n",
    "\n",
    "    return response.translations[0].translated_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fIPcn5dZ7O-b"
   },
   "source": [
    "## Classificação de texto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l2eDjxvafo5W"
   },
   "source": [
    "Na seção abaixo, você explorará prompts zero-shot, few-shot e alguns tipos comuns de tarefas de classificação de texto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bC3qkPZ8jFkY"
   },
   "source": [
    "### Prompt zero-shot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W8RFu2ZX_o_y"
   },
   "source": [
    "O prompt zero-shot é onde você não fornece exemplos com rótulos e conta com o LLM para fazer a classificação por conta própria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_uNNGhC_e1nZ"
   },
   "outputs": [],
   "source": [
    "prompt = traduza(\"\"\"\n",
    "Ordene el texto a continuación:\\n\n",
    "texto: \"Hoy vi un animal peludo en el parque - Tenía una cola larga y ojos grandes\".\n",
    "clase: perros, gatos\n",
    "\"\"\", \"en\")\n",
    "\n",
    "print(\n",
    "    traduza(generation_model.predict(prompt=prompt, max_output_tokens=256, temperature=0.1).text, \"es\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tjl-tckTjK2B"
   },
   "source": [
    "### Prompt few-shot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5UC0w7n5_o_z"
   },
   "source": [
    "Com prompt few-shot, você fornece exemplos para o modelo PaLM e espera que ele faça a classificação com base nos exemplos fornecidos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b8jYL-hBjMtW"
   },
   "outputs": [],
   "source": [
    "prompt = traduza(\"\"\"\n",
    "¿Cuál es el tema de un titular de noticias en particular? \\n\n",
    "- negocio \\n\n",
    "- entretenimiento \\n\n",
    "- salud \\n\n",
    "- deportes \\n\n",
    "- tecnología \\n\\n\n",
    "\n",
    "Texto: Revisión práctica de expertos del Pixel 7 Pro. \\n\n",
    "La respuesta es: tecnología \\n\n",
    "\n",
    "Texto: ¿Dejar de fumar? \\norte\n",
    "La respuesta es: salud \\n\n",
    "\n",
    "Texto: ¿Pajaritos o cucos? Los 5 mejores consejos para golpear por debajo del par \\n\n",
    "La respuesta es: deportes \\n\n",
    "\n",
    "Texto: El alivio del aumento del salario mínimo local parece más remoto\\n\n",
    "La respuesta es: negocio \\n\n",
    "\n",
    "Texto: No adivinarán quién acaba de llegar a Bari, Italia, para el estreno de la película. \\n\n",
    "La respuesta es:\n",
    "\"\"\", \"en\")\n",
    "\n",
    "print(\n",
    "    traduza(generation_model.predict(prompt=prompt, max_output_tokens=256, temperature=0.1).text, \"es\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WaiMLs4SjNLi"
   },
   "source": [
    "### Outros exemplos de classificação"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LhkcRWrh_o_0"
   },
   "source": [
    "Explore alguns prompts de classificação de texto mais comuns abaixo, todos baseados em prompts zero-shot. Você também pode transformar alguns deles em prompts few-shot, fornecendo seus próprios exemplos personalizados de texto e as classes de saída associadas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8tEjKEAtXjf8"
   },
   "source": [
    "#### Classificação de tópico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bCnuQRVSXmyr"
   },
   "outputs": [],
   "source": [
    "prompt = traduza(\"\"\"\n",
    "Clasifique un fragmento de texto en uno de varios temas predefinidos, como deportes, política o entretenimiento. \\n\n",
    "texto: El presidente Biden visitará la India en el mes de marzo para discutir algunas oportunidades. \\n\n",
    "clase:\n",
    "\"\"\", \"en\")\n",
    "\n",
    "print(\n",
    "    traduza(generation_model.predict(prompt=prompt, max_output_tokens=256, temperature=0.1).text, \"es\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rB6rZD-6YAkC"
   },
   "source": [
    "####  Detecção de spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OfyHvhBfX_P_"
   },
   "outputs": [],
   "source": [
    "prompt = traduza(\"\"\"\n",
    "Dado un correo electrónico, clasificarlo como spam o no spam. \\norte\n",
    "correo electrónico: hola usuario,\\n\n",
    "        ha sido seleccionado como ganador de la lotería y puede ganar hasta 1 millón de dólares. \\norte\n",
    "        comparta amablemente sus datos bancarios y podemos proceder desde allí. \\n\\n\n",
    "\n",
    "        de, \\n\n",
    "        Departamento oficial de lotería de EE. UU.\n",
    "\"\"\", \"en\")\n",
    "\n",
    "print(\n",
    "    generation_model.predict(prompt=prompt, max_output_tokens=256, temperature=0.1).text\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cHKcxx0TYrGv"
   },
   "source": [
    "#### Reconhecimento de intenção"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DsseGvWNYvK3"
   },
   "outputs": [],
   "source": [
    "prompt = traduza(\"\"\"\n",
    "Dada la entrada de un usuario, clasifique su intención, como \"buscar información\", \"hacer una reserva\" o \"hacer un pedido\". \\norte\n",
    "Entrada del usuario: Hola, ¿puedes reservar una mesa para dos en Google Restaurant para el 1 de mayo?\n",
    "\"\"\", \"en\")\n",
    "\n",
    "print(\n",
    "    traduza(generation_model.predict(prompt=prompt, max_output_tokens=256, temperature=0.1).text, \"es\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "stsfav5aZtqV"
   },
   "source": [
    "#### Identificação de linguagem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "88TqQSXIZxts"
   },
   "outputs": [],
   "source": [
    "prompt = traduza(\"\"\"\n",
    "Dado un fragmento de texto, clasificar el idioma en el que está escrito. \\n\n",
    "texto: Selam nasıl gidiyor?\n",
    "idioma:\n",
    "\"\"\", \"en\")\n",
    "\n",
    "print(\n",
    "    traduza(generation_model.predict(prompt=prompt, max_output_tokens=256, temperature=0.1).text, \"es\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Z_jmrhOZ15J"
   },
   "source": [
    "#### Detecção de toxicidade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Umloy-o1Z5us"
   },
   "outputs": [],
   "source": [
    "prompt = traduza(\"\"\"\n",
    "Dado un fragmento de texto, clasifícalo como tóxico o no tóxico. \\n\n",
    "texto: Intenta que puedas.\n",
    "\"\"\", \"en\")\n",
    "\n",
    "print(\n",
    "    traduza(generation_model.predict(prompt=prompt, max_output_tokens=256, temperature=0.1).text, \"es\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rTH5MeiVZ6Hr"
   },
   "source": [
    "#### Detecção de emoção"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u5ETwBSrZ-Xn"
   },
   "outputs": [],
   "source": [
    "prompt = traduza(\"\"\"\n",
    "Dado un fragmento de texto, califique la emoción que transmite, como \"felicidad\" o \"ira\". \\n\n",
    "texto: estoy muy feliz por la noticia de ayer\n",
    "\"\"\", \"en\")\n",
    "\n",
    "print(\n",
    "    traduza(generation_model.predict(prompt=prompt, max_output_tokens=256, temperature=0.5).text, \"es\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7ddaadac64c7"
   },
   "source": [
    "### Avaliação"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d5e2266cb257"
   },
   "source": [
    "Você pode avaliar as saídas da tarefa de classificação de texto se as classes de informações básicas estiverem disponíveis. Para mostrar como isso funciona, comece criando um dataframe simples com análises de produtos e o sentimento de verdade."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b0e04a03f24f"
   },
   "outputs": [],
   "source": [
    "review_data = {\n",
    "    \"review\": [\n",
    "         \"Me encanta este producto. ¡Tiene todo lo que busco!\",\n",
    "         \"Todo lo que puedo decir es que estará feliz después de comprar este producto\",\n",
    "         \"es demasiado caro y no vale la pena el precio\",\n",
    "         \"Él está bien. Ni bien ni tan mal\",\n",
    "    ],\n",
    "    \"sentiment_groundtruth\": [\"positivo\", \"positivo\", \"negativo\", \"neutral\"],\n",
    "}\n",
    "\n",
    "review_data_df = pd.DataFrame(review_data)\n",
    "review_data_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "088327f41a26"
   },
   "source": [
    "Agora que você tem os dados com avaliações e sentimentos como rótulos de informações básicas, pode chamar o modelo de geração de texto para cada linha de avaliação usando a função `apply`. Cada linha usará o prompt na coluna `review` para prever o sentimento usando a API PaLM e armazenará os resultados na coluna `sentiment_prediction`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0fb691b6c721"
   },
   "outputs": [],
   "source": [
    "def get_sentiment(row):\n",
    "    prompt = traduza(f\"\"\"Califique la opinión de las reseñas a continuación como \"positiva\", \"neutral\" o \"negativa\". \\n\\n\n",
    "                 revisión: {row} \\n\n",
    "                 sentimiento:\n",
    "               \"\"\", \"en\")\n",
    "    response = generation_model.predict(prompt=prompt).text\n",
    "    return traduza(response, \"es\")\n",
    "\n",
    "\n",
    "review_data_df[\"sentiment_prediction\"] = review_data_df[\"review\"].apply(get_sentiment)\n",
    "review_data_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "908c72bdf4c7"
   },
   "source": [
    "No final, você pode chamar a função `classification_report` do móulo `scikit-learn` para medir a precisão e outras métricas de classificação, passando sentimentos de verdade absoluta `sentiment_groundtruth` e sentimento previsto `sentiment_prediction`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7f22690ae395"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(\n",
    "    classification_report(\n",
    "        review_data_df[\"sentiment_groundtruth\"], review_data_df[\"sentiment_prediction\"]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "text_classification.ipynb",
   "toc_visible": true
  },
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-8.m107",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-8:m107"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "local-python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
